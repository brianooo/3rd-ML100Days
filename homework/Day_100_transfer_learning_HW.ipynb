{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "礙於不是所有同學都有 GPU ，這邊的範例使用的是簡化版本的 ResNet，確保所有同學都能夠順利訓練!\n",
    "\n",
    "\n",
    "最後一天的作業請閱讀這篇非常詳盡的[文章](https://blog.gtwang.org/programming/keras-resnet-50-pre-trained-model-build-dogs-cats-image-classification-system/)，基本上已經涵蓋了所有訓練　CNN 常用的技巧，請使用所有學過的訓練技巧，盡可能地提高 Cifar-10 的 test data 準確率，截圖你最佳的結果並上傳來完成最後一次的作業吧!\n",
    "\n",
    "另外這些技巧在 Kaggle 上也會被許多人使用，更有人會開發一些新的技巧，例如使把預訓練在 ImageNet 上的模型當成 feature extractor 後，再拿擷取出的特徵重新訓練新的模型，這些技巧再進階的課程我們會在提到，有興趣的同學也可以[參考](https://www.kaggle.com/insaff/img-feature-extraction-with-pretrained-resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ia3wYstDsEN"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "#from resnet_builder import resnet # 這是從 resnet_builder.py 中直接 import 撰寫好的 resnet 函數\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJJJyxyTWgN3"
   },
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "def resnet(input_shape, depth=29, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=256, activation='relu')(x)\n",
    "    y = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3760,
     "status": "ok",
     "timestamp": 1577336459513,
     "user": {
      "displayName": "Brian Lin",
      "photoUrl": "",
      "userId": "12010684563378800672"
     },
     "user_tz": -480
    },
    "id": "nGsBFm5OEHoU",
    "outputId": "9e6066fc-ee80-48a1-e454-80d81c4aba9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# 讀取資料集並作前處理\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWJoq9qLXCcp"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=20, \n",
    "                             width_shift_range=0.2, height_shift_range=0.2,\n",
    "                             shear_range=0.2, zoom_range=0.2,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 895050,
     "status": "ok",
     "timestamp": 1577339193773,
     "user": {
      "displayName": "Brian Lin",
      "photoUrl": "",
      "userId": "12010684563378800672"
     },
     "user_tz": -480
    },
    "id": "uo76Hr30EOcT",
    "outputId": "a96908ee-de26-46ad-d408-b54abf0bb610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 32, 32, 16)   448         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchN (None, 32, 32, 16)   64          conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_526 (Activation)     (None, 32, 32, 16)   0           batch_normalization_281[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 32, 32, 16)   272         activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchN (None, 32, 32, 16)   64          conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_527 (Activation)     (None, 32, 32, 16)   0           batch_normalization_282[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 32, 32, 16)   2320        activation_527[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_283 (BatchN (None, 32, 32, 16)   64          conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_528 (Activation)     (None, 32, 32, 16)   0           batch_normalization_283[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 32, 32, 64)   1088        activation_526[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 32, 32, 64)   1088        activation_528[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_171 (Add)                   (None, 32, 32, 64)   0           conv2d_315[0][0]                 \n",
      "                                                                 conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_284 (BatchN (None, 32, 32, 64)   256         add_171[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_529 (Activation)     (None, 32, 32, 64)   0           batch_normalization_284[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 32, 32, 16)   1040        activation_529[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_285 (BatchN (None, 32, 32, 16)   64          conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_530 (Activation)     (None, 32, 32, 16)   0           batch_normalization_285[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 32, 32, 16)   2320        activation_530[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_286 (BatchN (None, 32, 32, 16)   64          conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_531 (Activation)     (None, 32, 32, 16)   0           batch_normalization_286[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 32, 32, 64)   1088        activation_531[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_172 (Add)                   (None, 32, 32, 64)   0           add_171[0][0]                    \n",
      "                                                                 conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_287 (BatchN (None, 32, 32, 64)   256         add_172[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_532 (Activation)     (None, 32, 32, 64)   0           batch_normalization_287[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 32, 32, 16)   1040        activation_532[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_288 (BatchN (None, 32, 32, 16)   64          conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_533 (Activation)     (None, 32, 32, 16)   0           batch_normalization_288[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 32, 32, 16)   2320        activation_533[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_289 (BatchN (None, 32, 32, 16)   64          conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_534 (Activation)     (None, 32, 32, 16)   0           batch_normalization_289[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_321 (Conv2D)             (None, 32, 32, 64)   1088        activation_534[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_173 (Add)                   (None, 32, 32, 64)   0           add_172[0][0]                    \n",
      "                                                                 conv2d_321[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_290 (BatchN (None, 32, 32, 64)   256         add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_535 (Activation)     (None, 32, 32, 64)   0           batch_normalization_290[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_322 (Conv2D)             (None, 16, 16, 64)   4160        activation_535[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_291 (BatchN (None, 16, 16, 64)   256         conv2d_322[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_536 (Activation)     (None, 16, 16, 64)   0           batch_normalization_291[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_323 (Conv2D)             (None, 16, 16, 64)   36928       activation_536[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_292 (BatchN (None, 16, 16, 64)   256         conv2d_323[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_537 (Activation)     (None, 16, 16, 64)   0           batch_normalization_292[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 16, 16, 128)  8320        add_173[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_324 (Conv2D)             (None, 16, 16, 128)  8320        activation_537[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_174 (Add)                   (None, 16, 16, 128)  0           conv2d_325[0][0]                 \n",
      "                                                                 conv2d_324[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 16, 16, 128)  512         add_174[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_538 (Activation)     (None, 16, 16, 128)  0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 16, 16, 64)   8256        activation_538[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 16, 16, 64)   256         conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_539 (Activation)     (None, 16, 16, 64)   0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 16, 16, 64)   36928       activation_539[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 16, 16, 64)   256         conv2d_327[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_540 (Activation)     (None, 16, 16, 64)   0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 16, 16, 128)  8320        activation_540[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_175 (Add)                   (None, 16, 16, 128)  0           add_174[0][0]                    \n",
      "                                                                 conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 16, 16, 128)  512         add_175[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_541 (Activation)     (None, 16, 16, 128)  0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 16, 16, 64)   8256        activation_541[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 16, 16, 64)   256         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_542 (Activation)     (None, 16, 16, 64)   0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 16, 16, 64)   36928       activation_542[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 16, 16, 64)   256         conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_543 (Activation)     (None, 16, 16, 64)   0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 16, 16, 128)  8320        activation_543[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_176 (Add)                   (None, 16, 16, 128)  0           add_175[0][0]                    \n",
      "                                                                 conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 16, 16, 128)  512         add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_544 (Activation)     (None, 16, 16, 128)  0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 8, 8, 128)    16512       activation_544[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 8, 8, 128)    512         conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_545 (Activation)     (None, 8, 8, 128)    0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 8, 8, 128)    147584      activation_545[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 8, 8, 128)    512         conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_546 (Activation)     (None, 8, 8, 128)    0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 8, 8, 256)    33024       add_176[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 8, 8, 256)    33024       activation_546[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_177 (Add)                   (None, 8, 8, 256)    0           conv2d_335[0][0]                 \n",
      "                                                                 conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 8, 8, 256)    1024        add_177[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_547 (Activation)     (None, 8, 8, 256)    0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 8, 8, 128)    32896       activation_547[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 8, 8, 128)    512         conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_548 (Activation)     (None, 8, 8, 128)    0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 8, 8, 128)    147584      activation_548[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 8, 8, 128)    512         conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_549 (Activation)     (None, 8, 8, 128)    0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 8, 8, 256)    33024       activation_549[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_178 (Add)                   (None, 8, 8, 256)    0           add_177[0][0]                    \n",
      "                                                                 conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 8, 8, 256)    1024        add_178[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_550 (Activation)     (None, 8, 8, 256)    0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 8, 8, 128)    32896       activation_550[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 8, 8, 128)    512         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_551 (Activation)     (None, 8, 8, 128)    0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 8, 8, 128)    147584      activation_551[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 8, 8, 128)    512         conv2d_340[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_552 (Activation)     (None, 8, 8, 128)    0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 8, 8, 256)    33024       activation_552[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_179 (Add)                   (None, 8, 8, 256)    0           add_178[0][0]                    \n",
      "                                                                 conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 8, 8, 256)    1024        add_179[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_553 (Activation)     (None, 8, 8, 256)    0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 4, 4, 256)    0           activation_553[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 4096)         0           average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 256)          1048832     flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 10)           2570        dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,897,834\n",
      "Trainable params: 1,892,618\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "782/781 [==============================] - 81s 103ms/step - loss: 2.3535 - acc: 0.3062 - val_loss: 1.9769 - val_acc: 0.3999\n",
      "Epoch 2/50\n",
      "782/781 [==============================] - 54s 70ms/step - loss: 1.9305 - acc: 0.3983 - val_loss: 2.2613 - val_acc: 0.3677\n",
      "Epoch 3/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.7319 - acc: 0.4511 - val_loss: 1.9317 - val_acc: 0.4689\n",
      "Epoch 4/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 1.6259 - acc: 0.4887 - val_loss: 1.4322 - val_acc: 0.5468\n",
      "Epoch 5/50\n",
      "782/781 [==============================] - 54s 68ms/step - loss: 1.5482 - acc: 0.5151 - val_loss: 1.2543 - val_acc: 0.6205\n",
      "Epoch 6/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.4732 - acc: 0.5439 - val_loss: 1.3268 - val_acc: 0.5933\n",
      "Epoch 7/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.4243 - acc: 0.5613 - val_loss: 1.5727 - val_acc: 0.5524\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.3056 - acc: 0.6039 - val_loss: 1.2484 - val_acc: 0.6223\n",
      "Epoch 9/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.2595 - acc: 0.6233 - val_loss: 1.1450 - val_acc: 0.6629\n",
      "Epoch 10/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.2191 - acc: 0.6361 - val_loss: 1.0821 - val_acc: 0.6873\n",
      "Epoch 11/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.1876 - acc: 0.6503 - val_loss: 1.0465 - val_acc: 0.6949\n",
      "Epoch 12/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 1.1528 - acc: 0.6609 - val_loss: 1.3299 - val_acc: 0.6408\n",
      "Epoch 13/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 1.1261 - acc: 0.6696 - val_loss: 0.9769 - val_acc: 0.7185\n",
      "Epoch 14/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 1.0914 - acc: 0.6877 - val_loss: 0.9873 - val_acc: 0.7207\n",
      "Epoch 15/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 1.0671 - acc: 0.6951 - val_loss: 1.0472 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 16/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.9841 - acc: 0.7270 - val_loss: 0.9505 - val_acc: 0.7432\n",
      "Epoch 17/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.9518 - acc: 0.7375 - val_loss: 0.8724 - val_acc: 0.7705\n",
      "Epoch 18/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.9324 - acc: 0.7443 - val_loss: 0.7794 - val_acc: 0.7875\n",
      "Epoch 19/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.9039 - acc: 0.7552 - val_loss: 1.0415 - val_acc: 0.7214\n",
      "Epoch 20/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.8935 - acc: 0.7585 - val_loss: 0.7407 - val_acc: 0.7988\n",
      "Epoch 21/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.8733 - acc: 0.7656 - val_loss: 0.8732 - val_acc: 0.7718\n",
      "Epoch 22/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.8613 - acc: 0.7689 - val_loss: 0.8303 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 23/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.8247 - acc: 0.7800 - val_loss: 0.7670 - val_acc: 0.7983\n",
      "Epoch 24/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.7994 - acc: 0.7890 - val_loss: 0.7235 - val_acc: 0.8115\n",
      "Epoch 25/50\n",
      "782/781 [==============================] - 53s 67ms/step - loss: 0.7871 - acc: 0.7943 - val_loss: 0.7308 - val_acc: 0.8143\n",
      "Epoch 26/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7834 - acc: 0.7948 - val_loss: 0.8568 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 27/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7544 - acc: 0.8032 - val_loss: 0.7541 - val_acc: 0.8046\n",
      "Epoch 28/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7450 - acc: 0.8073 - val_loss: 0.6793 - val_acc: 0.8272\n",
      "Epoch 29/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7397 - acc: 0.8086 - val_loss: 0.7452 - val_acc: 0.8110\n",
      "Epoch 30/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7334 - acc: 0.8106 - val_loss: 0.7614 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 31/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.7197 - acc: 0.8153 - val_loss: 0.6738 - val_acc: 0.8306\n",
      "Epoch 32/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7136 - acc: 0.8184 - val_loss: 0.6748 - val_acc: 0.8291\n",
      "Epoch 33/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7110 - acc: 0.8184 - val_loss: 0.6677 - val_acc: 0.8325\n",
      "Epoch 34/50\n",
      "782/781 [==============================] - 56s 72ms/step - loss: 0.7114 - acc: 0.8180 - val_loss: 0.7177 - val_acc: 0.8219\n",
      "Epoch 35/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.7048 - acc: 0.8234 - val_loss: 0.6749 - val_acc: 0.8304\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 36/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6990 - acc: 0.8217 - val_loss: 0.6637 - val_acc: 0.8338\n",
      "Epoch 37/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6879 - acc: 0.8275 - val_loss: 0.6550 - val_acc: 0.8339\n",
      "Epoch 38/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6943 - acc: 0.8243 - val_loss: 0.6251 - val_acc: 0.8433\n",
      "Epoch 39/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6945 - acc: 0.8238 - val_loss: 0.6357 - val_acc: 0.8418\n",
      "Epoch 40/50\n",
      "782/781 [==============================] - 53s 68ms/step - loss: 0.6876 - acc: 0.8273 - val_loss: 0.6308 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 41/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6879 - acc: 0.8264 - val_loss: 0.6518 - val_acc: 0.8375\n",
      "Epoch 42/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6837 - acc: 0.8265 - val_loss: 0.6376 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 43/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6828 - acc: 0.8268 - val_loss: 0.6510 - val_acc: 0.8377\n",
      "Epoch 44/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6784 - acc: 0.8269 - val_loss: 0.6564 - val_acc: 0.8373\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 45/50\n",
      "782/781 [==============================] - 58s 74ms/step - loss: 0.6760 - acc: 0.8303 - val_loss: 0.6529 - val_acc: 0.8372\n",
      "Epoch 46/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6799 - acc: 0.8284 - val_loss: 0.6466 - val_acc: 0.8388\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 47/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6766 - acc: 0.8297 - val_loss: 0.6475 - val_acc: 0.8383\n",
      "Epoch 48/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6803 - acc: 0.8279 - val_loss: 0.6520 - val_acc: 0.8376\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 49/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6805 - acc: 0.8292 - val_loss: 0.6477 - val_acc: 0.8389\n",
      "Epoch 50/50\n",
      "782/781 [==============================] - 54s 69ms/step - loss: 0.6764 - acc: 0.8296 - val_loss: 0.6454 - val_acc: 0.8392\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n"
     ]
    }
   ],
   "source": [
    "# 相關參數設定\n",
    "batch_size = 64 # batch 的大小，如果出現 OOM error，請降低這個值\n",
    "num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n",
    "epochs = 50 # 訓練的 epochs 數量\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.5, min_lr=1e-12, monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "# 建立 ResNet 模型\n",
    "model = resnet(input_shape=(32,32,3)) \n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(x_train)/batch_size, epochs=epochs, verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3808,
     "status": "ok",
     "timestamp": 1577339197564,
     "user": {
      "displayName": "Brian Lin",
      "photoUrl": "",
      "userId": "12010684563378800672"
     },
     "user_tz": -480
    },
    "id": "EG0MRYrUs2mF",
    "outputId": "48671c65-f7d9-446c-aa6c-83f103bb05d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6454338319301606\n",
      "Test accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Day100.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
